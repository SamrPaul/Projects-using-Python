{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import mkdir, walk\n",
    "from pdfminer.high_level import extract_text\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from FileDetails import FileDetails\n",
    "\n",
    "import bs4 as bs\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "*****************\n",
    "  UTIL FUNCTIONS\n",
    "*****************\n",
    "\"\"\"\n",
    "def get_file_list_from_folder(folder_name):\n",
    "    file_list = []\n",
    "    try:\n",
    "        for(files) in walk(str(folder_name),topdown=True):\n",
    "            file_list = files[len(files) - 1]\n",
    "            # Code for cleaning up the file list remove all files starting with ._\n",
    "            for i in file_list:\n",
    "                if i.startswith(\"._\"):\n",
    "                    file_list.remove(i)\n",
    "        return file_list\n",
    "    except:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "***************************\n",
    "  CONVERT PDF TO TXT CODE\n",
    "***************************\n",
    "\"\"\"\n",
    "def remove_pdf_extension(string):\n",
    "    string = str(string)\n",
    "    string = string.split('.')\n",
    "    return string[0]\n",
    "\n",
    "def extract_text1(pdf_name):\n",
    "    text = extract_text(pdf_name)\n",
    "    return text\n",
    "\n",
    "def save_in_txt_files(pdf_path):\n",
    "    \"\"\"\n",
    "    Points to be noted while passing the pdf_name...\n",
    "        1. The pdf_name is the pdf path like => Repository/pdf_name.pdf\n",
    "    \"\"\"\n",
    "    print(pdf_path)\n",
    "    path_splits = pdf_path.split(\"/\")\n",
    "    filepath = str(\"output/\"+remove_pdf_extension(path_splits[len(path_splits)-1])+\".txt\")\n",
    "    try:\n",
    "        file = open(r\"{}\".format(filepath),\"w\")\n",
    "        text = extract_text1(pdf_path)\n",
    "        text = str(text)\n",
    "        file.write(text)\n",
    "        file.close()\n",
    "        return text\n",
    "    except:\n",
    "        mkdir('output')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "*************************\n",
    "    EXTRACT HEADINGS\n",
    "*************************\n",
    "\"\"\"\n",
    "def extract_headings(text):\n",
    "    \"\"\"\n",
    "        This function will find any headings in the text file. The regular expression finds the\n",
    "    string with a number at the start following a period, following a space, following a alphanumberic characters\n",
    "\n",
    "    Args: String - text\n",
    "    Return: List(String)\n",
    "    \"\"\"\n",
    "    line_list = []\n",
    "    heading_list = []\n",
    "    line_list = str(text).split(\"\\n\")\n",
    "    for i in line_list:\n",
    "        matchOject = re.compile(r'^[0-9]\\.+\\s+\\w+[\\s\\w]+')\n",
    "        temp_heading = matchOject.findall(i)\n",
    "        if temp_heading != []:\n",
    "            for i in temp_heading:\n",
    "                heading_list.append(i)\n",
    "    return heading_list\n",
    "\n",
    "def extract_heading_in_list(path_to_txt_file):\n",
    "    with open(path_to_txt_file,\"r\") as f:\n",
    "        content = f.read()\n",
    "        headings = extract_headings(content)\n",
    "        f.close()\n",
    "        return headings\n",
    "\n",
    "def get_headings_from_list(txt_file_list,txt_folder_name):\n",
    "    heading_frequency_dict = {}\n",
    "    replaceObject = re.compile(r'^[0-9]\\.+\\s+')\n",
    "    for i in txt_file_list:\n",
    "        heading_list = extract_heading_in_list(\"{0}/{1}\".format(txt_folder_name,i))\n",
    "        for j in heading_list:\n",
    "            processed_txt = replaceObject.split(j)\n",
    "            processed_heading = processed_txt[len(processed_txt) - 1].upper().strip()\n",
    "            if processed_heading in heading_frequency_dict:\n",
    "                heading_frequency_dict[processed_heading] += 1\n",
    "            else:\n",
    "                heading_frequency_dict[processed_heading] = 1\n",
    "    print(heading_frequency_dict,end='\\n')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "********************************************\n",
    "  EXTRACT REFERENCES AND SAVE IT IN FOLDER\n",
    "********************************************\n",
    "\"\"\"\n",
    "\n",
    "def extract_reference_from_txt_file(path_of_txt_file,source_folder_name):\n",
    "    txt_path = str(source_folder_name+\"/\"+path_of_txt_file)\n",
    "    txt_file = open(txt_path,'r')\n",
    "    ref_int = 0\n",
    "    references = str(\"references/\"+path_of_txt_file)\n",
    "    try:\n",
    "        ref_file = open(r\"{}\".format(references),\"w\")\n",
    "        for i in txt_file:\n",
    "            if ref_int == 1:\n",
    "                ref_file.write(i)\n",
    "            i = i.rstrip()\n",
    "\n",
    "            if re.search(\".*References|.*REFERENCES\",i):\n",
    "                ref_int=1\n",
    "    except:\n",
    "        try:\n",
    "            mkdir('references')\n",
    "        except:\n",
    "            print(\"directory found!\")\n",
    "    txt_file.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "********************************************\n",
    "  MATCHING REFERENCES WITH TITLE OF PAPERS\n",
    "********************************************\n",
    "\"\"\"\n",
    "# Problem Solution: https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n",
    "# Sentence Transformet: https://pypi.org/project/sentence-transformers/\n",
    "\n",
    "heading_find_regex = re.compile(r': [A-Za-z]+ .*')\n",
    "number_check_regex_type_1 = re.compile(r'[0-9]+\\.') # 21.\n",
    "number_check_regex_type_2 = re.compile(r'\\[[0-9]+\\]') # [2]\n",
    "\n",
    "def remove_starting_numbers(input_list):\n",
    "    temp_list = []\n",
    "    res_list = []\n",
    "    temp_list = input_list\n",
    "    for i in temp_list:\n",
    "        sentence = i.split(' ')\n",
    "        if number_check_regex_type_1.search(str(sentence[0])):\n",
    "            sentence.remove(sentence[0])\n",
    "            sen = \" \".join(sentence)\n",
    "            res_list.append(sen)\n",
    "        elif number_check_regex_type_2.search(str(sentence[0])):\n",
    "            sentence.remove(sentence[0])\n",
    "            sen = \" \".join(sentence)\n",
    "            res_list.append(sen)\n",
    "        else:\n",
    "            res_list.append(sentence)\n",
    "    \n",
    "    for i in res_list:\n",
    "        if i == ['']:\n",
    "            res_list.remove(i)\n",
    "    \n",
    "    return res_list\n",
    "\n",
    "def search_references(references_list,heading_list,model):\n",
    "    ref_temp = references_list\n",
    "    res_list = remove_starting_numbers(ref_temp)\n",
    "    # search_dict = dict()\n",
    "    for i in range(len(res_list)):\n",
    "        encode_i = model.encode(res_list[i],convert_to_tensor=True)\n",
    "        for j in heading_list:\n",
    "            cosine_scores = util.cos_sim(encode_i,j['Encoding_data'])\n",
    "            if cosine_scores[0][0] >= 0.7:\n",
    "                print(\"S1:{0} \\nS2:{1} \\n\\tScore: {2}\".format(res_list[i],j['Title'],cosine_scores[0][0]))\n",
    "\n",
    "def extract_references_from_file(file_path):\n",
    "    sentence_list = []\n",
    "    main_list = []\n",
    "    ref_list = []\n",
    "    match_only_list = []\n",
    "    temp = ''\n",
    "    txt_file = open(file_path,'r')\n",
    "    temp = ''.join(txt_file)\n",
    "    sentence_list = temp.splitlines(True)\n",
    "\n",
    "    # Removing extra new-line characters.\n",
    "    for i in sentence_list:\n",
    "        if i == '\\n':\n",
    "            sentence_list.remove(i)\n",
    "\n",
    "    for i in sentence_list:\n",
    "        search_str = i.split(' ')[0]\n",
    "        if number_check_regex_type_1.search(str(search_str)):\n",
    "            # print(i)\n",
    "            main_list.append((i,'MATCHED'))\n",
    "            match_only_list.append(i)\n",
    "            \n",
    "        elif number_check_regex_type_2.search(str(search_str)):\n",
    "            main_list.append((i,'MATCHED'))\n",
    "        else:\n",
    "            main_list.append((i,'NOT_MATCHED'))\n",
    "    \n",
    "    ref = ''\n",
    "    for i in main_list:\n",
    "        if i[1] == 'MATCHED':\n",
    "            ref_list.append(ref)\n",
    "            ref = i[0]\n",
    "        else:\n",
    "            ref += i[0]\n",
    "    ref_list.append(ref)       \n",
    "    \n",
    "    ref_temp = []\n",
    "    for i in ref_list:\n",
    "        ref_temp.append(i.replace(\"\\n\", \"\").replace(\"\\x0c\",\"\"))\n",
    "    \n",
    "    ref_list = ref_temp\n",
    "    return ref_list\n",
    "    \n",
    "\n",
    "def get_title_names(model):\n",
    "    json_file = open('REFERENCES.json')\n",
    "    data_array = json.load(json_file)\n",
    "    file_details_array = []\n",
    "    for i in data_array:\n",
    "        title_encode = model.encode(i['Title'],convert_to_tensor=True)\n",
    "        # print(title_encode)\n",
    "        file_details = FileDetails(i['Title'],i['Filename'],title_encode)\n",
    "        file_details_array.append(file_details)\n",
    "    return file_details_array\n",
    "\n",
    "# This function should run only once or whenever necessary\n",
    "def save_headings_to_pickle(model):\n",
    "    file_details_array = get_title_names(model)\n",
    "    processed_title = open('PROCESSED_REFERENCES.pkl','wb')\n",
    "    processed_list = []\n",
    "    for i in file_details_array:\n",
    "        processed_list.append({\"Title\": i.title,\"Path\": i.path, \"Encoding_data\": i.encoding_data})\n",
    "    pickle.dump(processed_list,processed_title)\n",
    "    processed_title.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "*********************\n",
    "  SUMMARY OF PAPERS\n",
    "*********************\n",
    "\"\"\"\n",
    "\n",
    "def Summary(line_list):\n",
    "    heading_list = []\n",
    "    def TextSummarization(res):\n",
    "        article = res\n",
    "\n",
    "        parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "        paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "        article_text = \"\"\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_text += p.text\n",
    "        # Removing Square Brackets and Extra Spaces\n",
    "        article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "        article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "        # Removing special characters and digits\n",
    "        formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "        formatted_article_text = re.sub(r'\\s+',' ', formatted_article_text)\n",
    "        sentence_list = nltk.sent_tokenize(article_text)\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        word_frequencies = {}\n",
    "        sentence_scores = {}\n",
    "        for word in nltk.word_tokenize(formatted_article_text):\n",
    "            if word not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "            maximum_frequncy = max(word_frequencies.values())\n",
    "            \n",
    "        \n",
    "        for word in word_frequencies.keys():\n",
    "            word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        for sent in sentence_list:\n",
    "            for word in nltk.word_tokenize(sent.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    if len(sent.split(' ')) < 30:\n",
    "                        if sent not in sentence_scores.keys():\n",
    "                            sentence_scores[sent] = word_frequencies[word]\n",
    "                        else:\n",
    "                            sentence_scores[sent] += word_frequencies[word]\n",
    "        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        output_file.write(summary)\n",
    "        print(summary)\n",
    "        \n",
    "    def textExtraction(new_heading,heading_lower):\n",
    "        c = [x for x in heading_lower if any(k in x for k in new_heading)]\n",
    "        inverse_index = { element: index for index, element in enumerate(c) }\n",
    "\n",
    "        #To find the index of the matched elements in the original heading list\n",
    "        z= [(index) for index, element in enumerate(heading_lower) if element in inverse_index]\n",
    "        #To extract the text under the sections\n",
    "        res = ''\n",
    "        for i in z:\n",
    "            p=heading_list[i]\n",
    "            if(p[3:]==\"CONCLUSION\" or p[3:]==\"conclusion\" or p[3:]==\"Conclusion\"):\n",
    "                j=heading_list[i]\n",
    "                k=\"REFERENCES\"\n",
    "            \n",
    "    # getting index of substrings\n",
    "                idx1 = line_list.index(j)\n",
    "                idx2 = line_list.index(k)\n",
    "                res = ''\n",
    "    # getting elements in between\n",
    "                for idx in range(idx1 + 1, idx2):\n",
    "                    res = res + line_list[idx]\n",
    "            else:\n",
    "                j=heading_list[i]\n",
    "                k=heading_list[i+1]\n",
    "            # getting index of substrings\n",
    "                idx1 = line_list.index(j)\n",
    "                idx2 = line_list.index(k)\n",
    "                res = ''\n",
    "            # getting elements in between\n",
    "                for idx in range(idx1 + 1, idx2):\n",
    "                    res = res + line_list[idx]\n",
    "                    print(\"res\",res)\n",
    "        section_extraction.write(res)\n",
    "        TextSummarization(res)\n",
    "    def synonymmatch(crct_heading,heading_list):\n",
    "        a = (map(lambda x: x.lower(), heading_list))\n",
    "        heading_lower= list(a)\n",
    "        synonyms=[\"abstract\",\"introduction\",\"literature-survey\",\"related work\",\"background\",\"methodology\",\"analysis\",\"comparison\",\"discussion\",\"results\",\"conclusion\",\"references\"]\n",
    "        new_heading=[]\n",
    "        synonyms.append(\"CONCLUSIONS\")\n",
    "    #to check and extract the matching headings and synonyms\n",
    "        for i in synonyms:\n",
    "            if i in crct_heading:\n",
    "                new_heading.append(i)\n",
    "        new_heading=list(set([x for x in crct_heading if any(b in x for b in synonyms)]))\n",
    "        textExtraction(new_heading,heading_lower)\n",
    "\n",
    "    def ExtractHeading():\n",
    "    \n",
    "    # TO extract the headings\n",
    "        for i in line_list:\n",
    "            matchOject = re.compile(r'^[0-9]\\.+\\s+\\w+[\\s\\w]+')\n",
    "            temp_heading = matchOject.findall(i)\n",
    "            if temp_heading != []:\n",
    "                for i in temp_heading:\n",
    "                    heading_list.append(i)\n",
    "        \n",
    "        crct_heading=[]\n",
    "\n",
    "    #To find the headings without the section number to match with the synonyms and convert it to lower case\n",
    "        for i in heading_list:\n",
    "            matchOject = re.compile(r'\\w+[\\s\\w]+')\n",
    "            temp_heading = matchOject.findall(i)\n",
    "            if temp_heading != []:\n",
    "                for i in temp_heading:\n",
    "                    crct_heading.append(i)\n",
    "        for i in range(len(crct_heading)):\n",
    "            crct_heading[i] = crct_heading[i].lower()\n",
    "        synonymmatch(crct_heading,heading_list)\n",
    "    ExtractHeading()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "****************\n",
    "  MAIN FUNCTION\n",
    "****************\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    option = int(input(\n",
    "        \"\"\"\n",
    "        What operation do you want to perform select the appropriate option\n",
    "            1. To setup the repository, convert the pdf data files to text\n",
    "            2. To extract headings and display its frequency\n",
    "            3. To extract references\n",
    "            4. To find references with title names\n",
    "            5. To find the summary of the input paper\n",
    "\n",
    "        Enter the number for performing the given operation:\n",
    "        \"\"\"\n",
    "    ))\n",
    "    if option == 1:\n",
    "        folder_name = str(input(\"Enter the folder name which has the pdf files: \"))\n",
    "        file_list = get_file_list_from_folder(folder_name)\n",
    "        for pdf_path in file_list:\n",
    "            save_in_txt_files(\"{0}/{1}\".format(folder_name,pdf_path))\n",
    "\n",
    "    elif option == 2:\n",
    "        # NOTE 1: GET YOUR PDF AND TXT FILES SETUP BEFORE RUNNING THIS OPTION\n",
    "        # NOTE 2: THIS IS A GENERATED FOLDER IT WILL BE GENERATED SO NEED OF CHANGING THE FOLDER NAME\n",
    "        folder_name = 'output'\n",
    "        file_list = get_file_list_from_folder(folder_name)\n",
    "        get_headings_from_list(file_list,folder_name)\n",
    "\n",
    "    elif option == 3:\n",
    "        folder_name = 'output'\n",
    "        file_list = get_file_list_from_folder(folder_name)\n",
    "        for i in file_list:\n",
    "            extract_reference_from_txt_file(i,folder_name)\n",
    "\n",
    "    elif option == 4:\n",
    "        print(\"Please wait... searching in our database!\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        a_file = open('PROCESSED_REFERENCES.pkl','rb')\n",
    "        output = pickle.load(a_file)\n",
    "        a_file.close()\n",
    "        reference_list = extract_references_from_file('references/2018Siva_OmnidirectionalMultisensoryPerceptionFusionLongTermPlaceRecognition.txt')\n",
    "        search_references(reference_list,output,model)\n",
    "    \n",
    "    elif option==5:\n",
    "        user_input=open('RS_2005.txt',encoding=\"utf-8\")\n",
    "        output_file=open('output.txt','w+')\n",
    "        section_extraction=open('output1.txt','w+')\n",
    "        text=user_input.read()\n",
    "        line_list = []\n",
    "        line_list = str(text).split(\"\\n\")\n",
    "        Summary(line_list)\n",
    "        output_file.close()\n",
    "        user_input.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
